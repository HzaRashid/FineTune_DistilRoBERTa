{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0266cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42939652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get movie reviews\n",
    "movie_reviews = pd.read_csv('IMDB Dataset.csv').filter(['review'])\n",
    "reviews = list(movie_reviews['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3010b3d7",
   "metadata": {},
   "source": [
    "#### Function to remove duplicate texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60e9fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(list_):\n",
    "    collect = set()\n",
    "    for item in list_:\n",
    "        if item not in collect:\n",
    "            collect.add(item)\n",
    "    collect = list(collect)\n",
    "    return collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812d1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = remove_duplicates(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484655c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get movie descriptions\n",
    "movie_synopses = pd.read_csv(r'movies_metadata.csv', low_memory=False).filter(['overview'])\n",
    "\n",
    "synopses = list(movie_synopses['overview'])\n",
    "synopses = remove_duplicates(synopses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e0756",
   "metadata": {},
   "source": [
    "#### Function to clean the reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42984a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_reviews(text_list):\n",
    "    for text in text_list:\n",
    "        # it will be too obvious when training the model if personal pronouns are included\n",
    "        text = re.sub('I', '', text)  \n",
    "        text = re.sub('my', '', text)\n",
    "        text = re.sub(\"I've\", '', text)\n",
    "        text = re.sub(\"I have\", '', text)\n",
    "        \n",
    "        # remove character sequences that only appear in reviews \n",
    "        text = re.sub('<br', '', text)\n",
    "        text = re.sub('/>', '', text)\n",
    "        text = re.sub(\"\\'\", \"'\", text)\n",
    "        text = text.strip(' ')\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27d8786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove null text\n",
    "reviews = list(filter(lambda x: not pd.isna(x), reviews))\n",
    "synopses = list(filter(lambda x: not pd.isna(x), synopses))\n",
    "\n",
    "# clean reviews\n",
    "reviews = clean_reviews(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a461d8cd",
   "metadata": {},
   "source": [
    "#### Apply labels & reduce no. characters to 240  ( Twitter's max char. count - and also speeds up training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcde90b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_reviews = [(reviews[i][:240], 'Subjective') for i in range(len(reviews))]\n",
    "labeled_synopses = [(synopses[i][:240], 'Objective') for i in range(len(synopses))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70150880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some text might be the same for first 240 characters, so remove duplicates again\n",
    "labeled_reviews = remove_duplicates(labeled_reviews)\n",
    "labeled_synopses = remove_duplicates(labeled_synopses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7abfcf06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49555, 44303)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_reviews), len(labeled_synopses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf18b6",
   "metadata": {},
   "source": [
    "### Reduce no. of texts to 5000 for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb55c0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_reviews = labeled_reviews[:5000]\n",
    "labeled_synopses = labeled_synopses[:5000]\n",
    "len(labeled_reviews), len(labeled_synopses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc31192d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge to one list\n",
    "labeled_data = labeled_reviews + labeled_synopses\n",
    "len(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1598e4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data = remove_duplicates(labeled_data)\n",
    "len(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "137497a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "499aea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "114f1e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df = df.rename(columns={0:'Text', 1:'labels'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ea60f",
   "metadata": {},
   "source": [
    "# Prepare Model and Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c8157",
   "metadata": {},
   "source": [
    "#### Load fast tokenizer for distil roberta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df492fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fast tokenizer for distil roberta \n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('distilroberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa84049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6c96db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, validtion, and test sets\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(df['Text'], \n",
    "                                                  df['labels'], \n",
    "                                                  train_size = 0.8, \n",
    "                                                  random_state = 24\n",
    "                                                 )\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, \n",
    "                                                    y_rem, \n",
    "                                                    test_size = 0.5, \n",
    "                                                    random_state = 24\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a07c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create torch dataset\n",
    "import torch\n",
    "\n",
    "class IMDbObjSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, pd_series, labels):\n",
    "        self.encodings = tokenizer(pd_series.tolist(), padding=True, truncation=True) \n",
    "        self.labels = list(labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        item = {key: torch.tensor(feature[idx]) for key, feature in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(0) if (self.labels[idx] == 'Objective') else torch.tensor(1)\n",
    "        return item\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86910186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62564309",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDbObjSet(X_train, y_train)\n",
    "val_dataset = IMDbObjSet(X_rem, y_rem)\n",
    "test_dataset = IMDbObjSet(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30eab9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03ce0f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify by comparing the label with the y_test label (0 if objective, 1 if subjective)\n",
    "test_data[0]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38110f3",
   "metadata": {},
   "source": [
    "#### So the text at test_data[0] should have the 'Objective' label (it is a synopsis). \n",
    "#### Verify below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23cc7a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: The life and career of vaudevillian and silent screen horror star Lon Chaney, his contentious relationship with his neurotic wife, and his premature death.\n",
      "\n",
      "LABEL: Objective\n"
     ]
    }
   ],
   "source": [
    "print('TEXT: ' + str(X_test.tolist()[0]))\n",
    "print()\n",
    "print('LABEL: ' + str(y_test.tolist()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3137edb0",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d713f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 8000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  45/1500 04:52 < 2:44:52, 0.15 it/s, Epoch 0.09/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.694800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.691400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.681500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('distilroberta-base') \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf1a31e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
