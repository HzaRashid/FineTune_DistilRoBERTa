{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0266cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e9fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(list_):\n",
    "    collect = set()\n",
    "    for item in list_:\n",
    "        if item not in collect:\n",
    "            collect.add(item)\n",
    "    collect = list(collect)\n",
    "    return collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93a95bf",
   "metadata": {},
   "source": [
    "### Function to clean the reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f8a84e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_reviews(text_list):\n",
    "    for text in text_list:\n",
    "        # it will be too obvious when training the model if personal pronouns are included\n",
    "        text = re.sub('I', '', text)  \n",
    "        text = re.sub('my', '', text)\n",
    "        text = re.sub(\"I've\", '', text)\n",
    "        text = re.sub(\"I have\", '', text)\n",
    "        \n",
    "        # remove character sequences that only appear in reviews \n",
    "        text = re.sub('<br', '', text)\n",
    "        text = re.sub('/>', '', text)\n",
    "        text = re.sub(\"\\'\", \"'\", text)\n",
    "        text = text.strip(' ')\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba4d7b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = pd.read_csv('IMDB Dataset.csv').filter(['review'])\n",
    "\n",
    "reviews = list(movie_reviews['review'])\n",
    "reviews = remove_duplicates(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484655c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_synopses = pd.read_csv(r'movies_metadata.csv', low_memory=False).filter(['overview'])\n",
    "\n",
    "synopses = list(movie_synopses['overview'])\n",
    "synopses = remove_duplicates(synopses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d8786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove null text\n",
    "reviews = list(filter(lambda x: not pd.isna(x), reviews))\n",
    "synopses = list(filter(lambda x: not pd.isna(x), synopses))\n",
    "\n",
    "# clean reviews\n",
    "reviews = clean_reviews(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23bea1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# down sample reviews to same length as synopses\n",
    "reviews = reviews[:len(synopses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37308ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 44307, 44307)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews) == len(synopses), len(reviews), len(synopses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcde90b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply labels & set max char. count to 240 -- like Twitter\n",
    "labeled_reviews = [(reviews[i][:240], 'Subjective') for i in range(len(reviews))]\n",
    "labeled_synopses = [(synopses[i][:240], 'Objective') for i in range(len(synopses))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70150880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some text might be the same for first 240 characters, so remove duplicates again\n",
    "labeled_reviews = remove_duplicates(labeled_reviews)\n",
    "labeled_synopses = remove_duplicates(labeled_synopses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7abfcf06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44287, 44303)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_reviews), len(labeled_synopses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb55c0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44287, 44287)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# down sample synopses to same length as reviews\n",
    "labeled_synopses = labeled_synopses[:len(labeled_reviews)]\n",
    "len(labeled_reviews), len(labeled_synopses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc31192d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88574"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge to one list\n",
    "labeled_data = labeled_reviews + labeled_synopses\n",
    "len(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1598e4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88574"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data = remove_duplicates(labeled_data)\n",
    "len(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "137497a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "499aea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "114f1e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df = df.rename(columns={0:'Text', 1:'Labels'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ea60f",
   "metadata": {},
   "source": [
    "# Prepare Model and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df492fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer for roberta base sentiment model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "rbs_model = f'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "tokenizer = AutoTokenizer.from_pretrained(rbs_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa84049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d6c96db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, validtion, and test sets\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(df['Text'], \n",
    "                                                  df['Labels'], \n",
    "                                                  train_size = 0.8, \n",
    "                                                  random_state = 24\n",
    "                                                 )\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, \n",
    "                                                    y_rem, \n",
    "                                                    test_size = 0.5, \n",
    "                                                    random_state = 24\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1a07c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create torch dataset\n",
    "import torch\n",
    "\n",
    "class IMDbObj(torch.utils.data.Dataset):\n",
    "    def __init__(self, pd_series, labels):\n",
    "        pd_series = list(pd_series)\n",
    "        labels = list(labels)\n",
    "        self.encodings = tokenizer(pd_series, truncation=True, padding=True)\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return {self.encodings[index]: self.labels[index]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "62564309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = IMDbObj(train_encodings, y_train)\n",
    "# val_dataset = IMDbObj(val_encodings, y_rem)\n",
    "test_dataset = IMDbObj(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "30eab9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "03ce0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_data = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c083e657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Encoding(num_tokens=273, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]): 'Subjective'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d713f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(rbs_model) \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf1a31e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
